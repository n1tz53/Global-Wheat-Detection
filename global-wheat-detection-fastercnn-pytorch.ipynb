{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport math\nimport time\nimport datetime\nfrom collections import deque, defaultdict\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport cv2 as cv\nimport  albumentations as A\nfrom PIL import Image\nfrom numba import jit\n\n\nimport torch\nimport torch.utils.data\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\nnp.random.seed(42)\ntorch.manual_seed(42)","execution_count":1,"outputs":[{"output_type":"execute_result","execution_count":1,"data":{"text/plain":"<torch._C.Generator at 0x7fe447b12b50>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/global-wheat-detection/train.csv')\ntrain.head()","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"    image_id  width  height                         bbox   source\n0  b6ab77fd7   1024    1024   [834.0, 222.0, 56.0, 36.0]  usask_1\n1  b6ab77fd7   1024    1024  [226.0, 548.0, 130.0, 58.0]  usask_1\n2  b6ab77fd7   1024    1024  [377.0, 504.0, 74.0, 160.0]  usask_1\n3  b6ab77fd7   1024    1024  [834.0, 95.0, 109.0, 107.0]  usask_1\n4  b6ab77fd7   1024    1024  [26.0, 144.0, 124.0, 117.0]  usask_1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>width</th>\n      <th>height</th>\n      <th>bbox</th>\n      <th>source</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>[834.0, 222.0, 56.0, 36.0]</td>\n      <td>usask_1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>[226.0, 548.0, 130.0, 58.0]</td>\n      <td>usask_1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>[377.0, 504.0, 74.0, 160.0]</td>\n      <td>usask_1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>[834.0, 95.0, 109.0, 107.0]</td>\n      <td>usask_1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>[26.0, 144.0, 124.0, 117.0]</td>\n      <td>usask_1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_img_id = ['a1321ca95', '409a8490c', 'd7a02151d', '9a30dd802', \\\n              '2cc75e9f5', '42e6efaaa', 'd067ac2b1', '41c0123cc']\n\ntrain = train[~(train['image_id'].isin(drop_img_id))]","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bbox_area(bbox, format='coco'):\n    \n    assert format in ['coco', 'pascal_voc']\n    \n    area = None\n    if format == 'coco':\n        area = bbox[2] * bbox[3]\n    if format == 'pascal_voc':\n        area = (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])\n        \n    return int(area)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['area'] = train['bbox'].apply(lambda arg: bbox_area(eval(arg)))\ntrain['log_area'] = np.log(train['area'])\nfig, ax = plt.subplots()\nn, bins, pathces = plt.hist(train['log_area'].values, density=1)\nax.set_xlabel('bins')\nax.set_ylabel('probability density')\nfig.tight_layout()\nplt.show()","execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"bins","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"array([ 0.69314718,  1.816811  ,  2.94047483,  4.06413865,  5.18780247,\n        6.31146629,  7.43513012,  8.55879394,  9.68245776, 10.80612158,\n       11.9297854 ])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[(train['log_area'] > 6.2) & (train['log_area'] < 12)]\ntrain.shape","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"(147312, 7)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nbins\ntrain[(train['log_area'] > 12)]\n'''","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"\"\\nbins\\ntrain[(train['log_area'] > 12)]\\n\""},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped = train.groupby(['source', 'image_id'])\n\nfig, ax = plt.subplots()\nn, bins, patches = plt.hist(grouped.apply(lambda group: len(group)).values, density=1)\nfig.tight_layout()\nplt.show()","execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"bins","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"array([  1. ,  12.5,  24. ,  35.5,  47. ,  58.5,  70. ,  81.5,  93. ,\n       104.5, 116. ])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"bbox_cat = {'A': [0, 20], 'B': [20, 40], 'C': [40, 60], 'D': [60, 80], 'E': [80, 100]}","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def map_bbox_cat(x):\n    \n    if x <= bbox_cat['A'][1]:\n        return 'A'\n    elif x <= bbox_cat['B'][1]:\n        return 'B'\n    elif x <= bbox_cat['C'][1]:\n        return 'C'\n    elif x <= bbox_cat['D'][1]:\n        return 'D'\n    else:\n        return 'E'\n        ","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped = grouped.apply(lambda group: map_bbox_cat(len(group)))\ngrouped = grouped.reset_index(name='box_cat')\ngrouped ","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"         source   image_id box_cat\n0     arvalis_1  00333207f       C\n1     arvalis_1  00e903abe       C\n2     arvalis_1  00ea5e5ee       C\n3     arvalis_1  01189a3c3       B\n4     arvalis_1  0126b7d11       B\n...         ...        ...     ...\n3360    usask_1  f5093f57d       B\n3361    usask_1  f6afe5443       B\n3362    usask_1  f8e590769       B\n3363    usask_1  fd5624913       B\n3364    usask_1  ffc870198       C\n\n[3365 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source</th>\n      <th>image_id</th>\n      <th>box_cat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>arvalis_1</td>\n      <td>00333207f</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>arvalis_1</td>\n      <td>00e903abe</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>arvalis_1</td>\n      <td>00ea5e5ee</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>arvalis_1</td>\n      <td>01189a3c3</td>\n      <td>B</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>arvalis_1</td>\n      <td>0126b7d11</td>\n      <td>B</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3360</th>\n      <td>usask_1</td>\n      <td>f5093f57d</td>\n      <td>B</td>\n    </tr>\n    <tr>\n      <th>3361</th>\n      <td>usask_1</td>\n      <td>f6afe5443</td>\n      <td>B</td>\n    </tr>\n    <tr>\n      <th>3362</th>\n      <td>usask_1</td>\n      <td>f8e590769</td>\n      <td>B</td>\n    </tr>\n    <tr>\n      <th>3363</th>\n      <td>usask_1</td>\n      <td>fd5624913</td>\n      <td>B</td>\n    </tr>\n    <tr>\n      <th>3364</th>\n      <td>usask_1</td>\n      <td>ffc870198</td>\n      <td>C</td>\n    </tr>\n  </tbody>\n</table>\n<p>3365 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped = grouped.groupby(['source', 'box_cat'])\ngrouped = grouped.apply(lambda group: group.sample(frac=0.2, random_state=42))\nval_image_ids = grouped.reset_index(drop=True)['image_id']\nlen(val_image_ids)","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"670"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train[~(train['image_id'].isin(val_image_ids))]\ntrain_df.head()","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"    image_id  width  height                         bbox   source   area  \\\n0  b6ab77fd7   1024    1024   [834.0, 222.0, 56.0, 36.0]  usask_1   2016   \n1  b6ab77fd7   1024    1024  [226.0, 548.0, 130.0, 58.0]  usask_1   7540   \n2  b6ab77fd7   1024    1024  [377.0, 504.0, 74.0, 160.0]  usask_1  11840   \n3  b6ab77fd7   1024    1024  [834.0, 95.0, 109.0, 107.0]  usask_1  11663   \n4  b6ab77fd7   1024    1024  [26.0, 144.0, 124.0, 117.0]  usask_1  14508   \n\n   log_area  \n0  7.608871  \n1  8.927977  \n2  9.379239  \n3  9.364177  \n4  9.582456  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>width</th>\n      <th>height</th>\n      <th>bbox</th>\n      <th>source</th>\n      <th>area</th>\n      <th>log_area</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>[834.0, 222.0, 56.0, 36.0]</td>\n      <td>usask_1</td>\n      <td>2016</td>\n      <td>7.608871</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>[226.0, 548.0, 130.0, 58.0]</td>\n      <td>usask_1</td>\n      <td>7540</td>\n      <td>8.927977</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>[377.0, 504.0, 74.0, 160.0]</td>\n      <td>usask_1</td>\n      <td>11840</td>\n      <td>9.379239</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>[834.0, 95.0, 109.0, 107.0]</td>\n      <td>usask_1</td>\n      <td>11663</td>\n      <td>9.364177</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>[26.0, 144.0, 124.0, 117.0]</td>\n      <td>usask_1</td>\n      <td>14508</td>\n      <td>9.582456</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_df = train[(train['image_id'].isin(val_image_ids))]\nval_df.head()","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"      image_id  width  height                         bbox   source   area  \\\n313  02b6f199c   1024    1024  [832.0, 727.0, 90.0, 159.0]  usask_1  14310   \n314  02b6f199c   1024    1024  [286.0, 724.0, 79.0, 106.0]  usask_1   8374   \n315  02b6f199c   1024    1024   [955.0, 869.0, 69.0, 74.0]  usask_1   5106   \n316  02b6f199c   1024    1024   [138.0, 297.0, 76.0, 89.0]  usask_1   6764   \n317  02b6f199c   1024    1024    [848.0, 0.0, 77.0, 100.0]  usask_1   7700   \n\n     log_area  \n313  9.568714  \n314  9.032887  \n315  8.538172  \n316  8.819370  \n317  8.948976  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>width</th>\n      <th>height</th>\n      <th>bbox</th>\n      <th>source</th>\n      <th>area</th>\n      <th>log_area</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>313</th>\n      <td>02b6f199c</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>[832.0, 727.0, 90.0, 159.0]</td>\n      <td>usask_1</td>\n      <td>14310</td>\n      <td>9.568714</td>\n    </tr>\n    <tr>\n      <th>314</th>\n      <td>02b6f199c</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>[286.0, 724.0, 79.0, 106.0]</td>\n      <td>usask_1</td>\n      <td>8374</td>\n      <td>9.032887</td>\n    </tr>\n    <tr>\n      <th>315</th>\n      <td>02b6f199c</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>[955.0, 869.0, 69.0, 74.0]</td>\n      <td>usask_1</td>\n      <td>5106</td>\n      <td>8.538172</td>\n    </tr>\n    <tr>\n      <th>316</th>\n      <td>02b6f199c</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>[138.0, 297.0, 76.0, 89.0]</td>\n      <td>usask_1</td>\n      <td>6764</td>\n      <td>8.819370</td>\n    </tr>\n    <tr>\n      <th>317</th>\n      <td>02b6f199c</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>[848.0, 0.0, 77.0, 100.0]</td>\n      <td>usask_1</td>\n      <td>7700</td>\n      <td>8.948976</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_img(img_path):\n    \n    img = cv.imread(img_path, cv.IMREAD_COLOR)\n    img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n    \n    return img","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class GlobalWheatDetectionDataset(torch.utils.data.Dataset):\n    \n    def __init__(self, root, df, transforms=None):\n        \n        self.root = root\n        self.transforms = transforms\n        # load all image files, sorting them to\n        # ensure that they are aligned\n        \n        self.imgs = sorted(list(df['image_id'].unique()))\n        \n        self.bboxs = {}\n        \n        for row in df.itertuples():\n            if row[1] not in self.bboxs:\n                self.bboxs[row.image_id] = [row.bbox]\n            else:\n                self.bboxs[row.image_id].append(row.bbox)\n        \n\n    def __getitem__(self, idx):\n        \n        # load images and convert to RGB\n        img_path = os.path.join(self.root, \"train\", self.imgs[idx] + '.jpg')\n        img = get_img(img_path).astype(np.float32)\n        \n        image_id = torch.tensor([idx])\n        \n        # get bounding box coordinates for each wheat head\n        num_objs = len(self.bboxs[self.imgs[idx].split('.')[0]])\n        boxes = []\n        \n        for box in self.bboxs[self.imgs[idx].split('.')[0]]:\n            box = eval(box)\n            xmin = box[0]\n            xmax = box[0] + box[2]\n            ymin = box[1]\n            ymax = box[1] + box[3]\n            boxes.append([xmin, ymin, xmax, ymax])\n            \n        if self.transforms:\n            aug = {'image': img,  'bboxes': boxes, 'labels': [1]*num_objs}\n            augmented = self.transforms(**aug)\n            img = augmented['image']\n            boxes = augmented['bboxes']\n\n        img = torch.from_numpy(img)\n        img = img.permute(2, 0, 1)\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        # there is only one class\n        labels = torch.ones((num_objs,), dtype=torch.int64)\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        \n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n        \n        return img, target\n\n    def __len__(self):\n        return len(self.imgs)","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dataset = GlobalWheatDetectionDataset('../input/global-wheat-detection/', train)\n# dataset[0]","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_transform(train):\n    \n    transforms = []\n    \n    if train:\n        # during training, randomly flip the training images\n        # and ground-truth for data augmentation\n        transforms.append(A.Flip(p=0.6))\n        transforms.append(A.OneOf([A.HueSaturationValue(p=0.9), A.RGBShift(p=0.9),\\\n                                   A.RandomBrightnessContrast(p=0.9)], p=0.9))\n        transforms.append(A.OneOf([A.GaussianBlur(blur_limit=5, p=0.9),\\\n                                   A.Blur(blur_limit=5, p=0.9)], p=0.01))\n        \n        transforms.append(A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=0, p=0.5))\n        \n    transforms.append(A.Resize(512, 512))\n    transforms.append(A.ToFloat(max_value=255.))\n    return A.Compose(transforms, bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\n# use our dataset and defined transformations\ndataset_train = GlobalWheatDetectionDataset('../input/global-wheat-detection/', train_df, get_transform(train=True))\ndataset_val = GlobalWheatDetectionDataset('../input/global-wheat-detection/', val_df, get_transform(train=False))\n\n# define training and validation data loaders\ndata_loader_train = torch.utils.data.DataLoader(\n    dataset_train, batch_size=32, shuffle=True, num_workers=4,\n    collate_fn=collate_fn)\n\ndata_loader_val = torch.utils.data.DataLoader(\n    dataset_val, batch_size=32, shuffle=False, num_workers=4,\n    collate_fn=collate_fn)","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Averager(object):\n    \n    def __init__(self, window_size=20, fmt='{avg:.4f} ({global_avg:.4f})'):\n        \n        self.values = deque(maxlen=window_size)\n        self.total = 0.\n        self.count = 0\n        self.fmt = fmt\n        pass\n    \n    def update(self, value):\n        \n        self.values.append(value)\n        self.total = self.total + value\n        self.count = self.count + 1\n        pass\n    \n    @property\n    def avg(self):\n        d = torch.tensor(self.values, dtype=torch.float32)\n        return d.mean().item()\n\n    @property\n    def global_avg(self):\n        return self.total / self.count\n    \n    def __str__(self):\n        return self.fmt.format(avg=self.avg, \\\n                               global_avg=self.global_avg)\n    \n\nclass TrainingLog(object):\n    \n    def __init__(self, delimiter=' '):\n        \n        self.metrics = defaultdict(Averager)\n        self.delimiter = delimiter\n        return\n    \n    def update(self, **loss_dict):\n        \n        for k, v in loss_dict.items():\n            if isinstance(v, torch.Tensor):\n                v = v.item()\n            assert isinstance(v, (float, int))\n            self.metrics[k].update(v)\n        return\n    \n    def __str__(self):\n        \n        loss_str = []\n        for k, v in self.metrics.items():\n            loss_str.append('{}: {}'.format(k, str(v)))\n        return self.delimiter.join(loss_str)\n    \n    def run_iterations(self, iterable, print_frq, header=''):\n        \n        i = 0\n        start = time.time()\n        end = time.time()\n        log_msg = self.delimiter.join([header, '[{0}/{1}]', 'eta: {eta}', \\\n                                       '{metrics}','time: {time}',\\\n                                       'data: {data}'])\n        data_time = Averager(fmt='{avg:.4f}')\n        iter_time = Averager(fmt='{avg:.4f}')\n        \n        for obj in iterable:\n            \n            data_time.update(time.time() - end)\n            yield obj\n            iter_time.update(time.time() - end)\n            \n            if i % print_frq == 0 or i == len(iterable) - 1:\n                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n                print(log_msg.format(i, len(iterable), eta=eta_string,\\\n                                     metrics=str(self), time=str(iter_time), \\\n                                     data=str(data_time)))\n            i = i + 1\n            end = time.time()\n        \n        total_time = time.time() - start\n        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n        print('{} Total time: {}'.format(header, total_time_str))\n        \n        return","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@jit(nopython=True)\ndef IoU(pred, ann, fmt='coco'):\n\n    axmin, aymin = pred[0], pred[1]\n    bxmin, bymin = ann[0], ann[1]\n    axmax, aymax = pred[2], pred[3]\n    bxmax, bymax = ann[2], ann[3]\n    if fmt == 'coco':\n        axmax = axmax + axmin\n        aymax = aymax + aymin\n        bxmax = bxmax + bxmin\n        bymax = bymax + bymin     \n    dx = min(axmax, bxmax) - max(axmin, bxmin) + 1\n    if dx < 0:\n        return 0\n    dy = min(aymax, bymax) - max(aymin, bymin) + 1\n    if dy < 0:\n        return 0\n    overlap = dx * dy        \n    area_union = (axmax - axmin + 1) * (aymax - aymin + 1) + \\\n                 (bxmax - bxmin + 1) * (bymax - bymin + 1) - \\\n                  overlap\n    return overlap / area_union\n\n@jit(nopython=True)\ndef precision_at(thld, preds, anns):\n    \n    thld = thld * 0.05 + 0.5\n    metrics = np.zeros(4)\n    available = np.ones(anns.shape[0])\n    for idx in range(preds.shape[0]):\n        pred = preds[idx]\n        mx_iou = 0.\n        mx_idx = -1\n        for jdx in range(anns.shape[0]):\n            ann = anns[jdx]\n            if available[jdx] == 1:\n                cur_iou = IoU(pred, ann)\n                if cur_iou > mx_iou:\n                    mx_iou = cur_iou\n                    mx_idx = jdx\n        if mx_iou >= thld:\n            available[mx_idx] = 0 \n            metrics[0] += 1 # True Positive\n        else:\n            metrics[1] += 1 # False Positive\n    metrics[2] += np.count_nonzero(available) # False negative\n    prec = metrics[0]/(metrics[0]+metrics[1]+metrics[2]) # precision\n    return prec\n\n@jit(nopython=True)\ndef precision_all(preds, anns):\n    \n    nthld = 6\n    prec_at = np.zeros(6)\n    for idx in range(nthld):\n        prec_at[idx] = precision_at(idx, preds, anns)\n    return prec_at, np.mean(prec_at)\n\n@jit(nopython=True)\ndef sort_boxes(boxes, scores):\n    qry = scores >= 0.5\n    scores = scores[qry]\n    scores = np.argsort(scores)[::-1]\n    return boxes[scores]\n    \nclass Evaluator(object):\n    \n    def __init__(self):\n        \n        self.nth = 6\n        self.th = [0.5, 0.55, 0.6, 0.65, 0.7, 0.75]\n        self.img_id_res = dict()\n        self.img_prec = []\n        self.map = 0.\n        return\n    \n    def update(self, img_id, preds, anns):\n        \n        out = precision_all(preds, anns)\n        self.img_id_res[str(img_id)] = out[0]\n        self.img_prec.append(out[1])\n        return True\n    \n    def update_batch(self, outputs, targets):\n        \n        for i, (output, target) in enumerate(zip(outputs, targets)):\n            image_id = target['image_id'].numpy().astype(np.int32)\n            p_boxes = output['boxes'].numpy().astype(np.int32)\n            p_scores = output['scores'].numpy().astype(np.float32)\n            t_boxes = target['boxes'].numpy().astype(np.int32)\n            p_boxes = sort_boxes(p_boxes, p_scores)\n            self.update(image_id, p_boxes, t_boxes)\n        return True\n            \n    def summary(self):\n\n        results = np.array(list(self.img_id_res.values()))\n        mAP = np.mean(results, axis=0)\n        for idx in range(mAP.shape[0]):\n            print('mAP @[IoU={0:.2f}] = {1:.4f}'.format(self.th[idx], mAP[idx]))\n        self.map = np.mean(np.array(self.img_prec))\n        print('mAP @[IoU=0.50:0.75] = {0:.4f}'.format(self.map))\n        return True            ","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nhttps://github.com/Bjarten/early-stopping-pytorch\n'''\n\nclass EarlyStopping:\n    \n    def __init__(self, patience=10, verbose=True, delta=0.0001, path='fasterrcnn_resnet50_fpn512x512.pth'):\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_mAP_min = np.Inf\n        self.delta = delta\n        self.path = path\n\n    def __call__(self, val_mAP, model):\n\n        score = val_mAP\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_mAP, model)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_mAP, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_mAP, model):\n        '''Saves model when validation loss decrease.'''\n        if self.verbose:\n            print(f'Validation mAP increased ({self.val_mAP_min:.4f} --> {val_mAP:.4f}).  Saving model ...')\n        torch.save(model.state_dict(), self.path)\n        self.val_mAP_min = val_mAP","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ndevice","execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# our dataset has two classes only - background and wheat head\nnum_classes = 2\nmodel_wt = '../input/fasterrcnn-resnet50-fpn-coco/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth'\n\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, \\\n                                                             pretrained_backbone=False,\\\n                                                             min_size=512, max_size=1024)\nmodel.load_state_dict(torch.load(model_wt))\n# get the number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n# move model to the right device\nmodel.to(device)\n\n# construct an optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\n\noptimizer = torch.optim.Adam(params, lr=0.0001, weight_decay=1e-5)\n\nlr_scheduler = None","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\ncheckpoint = '../input/gwd512x512/fasterrcnn_resnet50_fpnNBV22HW512x512.pth'\nmodel.load_state_dict(torch.load(checkpoint, map_location=torch.device(device)))\n'''","execution_count":27,"outputs":[{"output_type":"execute_result","execution_count":27,"data":{"text/plain":"\"\\ncheckpoint = '../input/gwd512x512/fasterrcnn_resnet50_fpnNBV22HW512x512.pth'\\nmodel.load_state_dict(torch.load(checkpoint, map_location=torch.device(device)))\\n\""},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's train\n\nnum_epochs = 30\naccumulation_steps = 2\nlr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max',\\\n                                                          patience=3,threshold_mode='abs',\\\n                                                          min_lr=1e-8,\n                                                          verbose=True)\nes = EarlyStopping()\n\nfor epoch in range(num_epochs):\n    \n    header = 'Epoch [{0}]'.format(epoch)\n    training_log = TrainingLog()\n    itr = 0\n    model.train()\n    \n    for images, targets in training_log.run_iterations(data_loader_train, 100, header):\n\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        \n        loss_dict = model(images, targets)\n        #itr = itr + 1\n        losses = sum(loss for loss in loss_dict.values())\n        # losses = losses / accumulation_steps\n        losses.backward()\n        #if itr % accumulation_steps == 0:\n        optimizer.step()\n        optimizer.zero_grad()\n        training_log.update(loss=losses, **loss_dict)\n    \n    with torch.no_grad():\n\n        evaluate = Evaluator()\n        model.eval()\n\n        header = 'Test'\n        validation_log = TrainingLog()\n\n        for images, targets in validation_log.run_iterations(data_loader_val, 100, header):\n            images = list(image.to(device) for image in images)\n            end = time.time()\n            outputs = model(images)\n            model_time = time.time() - end\n            outputs = [{k: v.to(\"cpu\") for k, v in t.items()} for t in outputs]\n            end = time.time()\n            evaluate.update_batch(outputs, targets)\n            evaluator_time = time.time() - end\n            validation_log.update(model_time=model_time, evaluator_time=evaluator_time)\n            \n        evaluate.summary()\n    lr_scheduler.step(evaluate.map)\n    es(evaluate.map, model)\n    if es.early_stop:\n        break","execution_count":28,"outputs":[{"output_type":"stream","text":"/opt/conda/conda-bld/pytorch_1587428398394/work/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n\tnonzero(Tensor input, *, Tensor out)\nConsider using one of the following signatures instead:\n\tnonzero(Tensor input, *, bool as_tuple)\n","name":"stderr"},{"output_type":"stream","text":"Epoch [0] [0/85] eta: 0:15:40 loss: 8.2019 (8.2019) loss_classifier: 0.6083 (0.6083) loss_box_reg: 0.1555 (0.1555) loss_objectness: 6.9878 (6.9878) loss_rpn_box_reg: 0.4504 (0.4504) time: 11.0688 data: 6.1210\nEpoch [0] [84/85] eta: 0:00:02 loss: 1.5985 (1.7212) loss_classifier: 0.4832 (0.4873) loss_box_reg: 0.3408 (0.3335) loss_objectness: 0.5441 (0.6604) loss_rpn_box_reg: 0.2305 (0.2398) time: 2.0154 data: 0.1924\nEpoch [0] Total time: 0:03:18\nTest [0/21] eta: 0:03:37 model_time: 1.0336 (1.0336) evaluator_time: 5.3795 (5.3795) time: 10.3368 data: 3.8185\nTest [20/21] eta: 0:00:01 model_time: 0.7479 (0.7615) evaluator_time: 0.0174 (0.2727) time: 1.0172 data: 0.1965\nTest Total time: 0:00:30\nmAP @[IoU=0.50] = 0.7429\nmAP @[IoU=0.55] = 0.7358\nmAP @[IoU=0.60] = 0.7260\nmAP @[IoU=0.65] = 0.7124\nmAP @[IoU=0.70] = 0.6917\nmAP @[IoU=0.75] = 0.6611\nmAP @[IoU=0.50:0.75] = 0.7116\nValidation mAP increased (inf --> 0.7116).  Saving model ...\nEpoch [1] [0/85] eta: 0:15:12 loss: 1.7626 (1.7626) loss_classifier: 0.5491 (0.5491) loss_box_reg: 0.3839 (0.3839) loss_objectness: 0.5847 (0.5847) loss_rpn_box_reg: 0.2450 (0.2450) time: 10.7339 data: 7.7463\nEpoch [1] [84/85] eta: 0:00:02 loss: 1.5073 (1.5647) loss_classifier: 0.4633 (0.4787) loss_box_reg: 0.3549 (0.3612) loss_objectness: 0.4676 (0.4973) loss_rpn_box_reg: 0.2215 (0.2275) time: 2.0376 data: 0.1972\nEpoch [1] Total time: 0:03:22\nTest [0/21] eta: 0:01:47 model_time: 1.0799 (1.0799) evaluator_time: 0.0520 (0.0520) time: 5.1069 data: 3.8685\nTest [20/21] eta: 0:00:01 model_time: 0.7737 (0.7882) evaluator_time: 0.0216 (0.0231) time: 1.0845 data: 0.2185\nTest Total time: 0:00:26\nmAP @[IoU=0.50] = 0.7474\nmAP @[IoU=0.55] = 0.7395\nmAP @[IoU=0.60] = 0.7299\nmAP @[IoU=0.65] = 0.7162\nmAP @[IoU=0.70] = 0.6965\nmAP @[IoU=0.75] = 0.6660\nmAP @[IoU=0.50:0.75] = 0.7159\nValidation mAP increased (0.7116 --> 0.7159).  Saving model ...\nEpoch [2] [0/85] eta: 0:14:04 loss: 1.4572 (1.4572) loss_classifier: 0.4501 (0.4501) loss_box_reg: 0.3302 (0.3302) loss_objectness: 0.4733 (0.4733) loss_rpn_box_reg: 0.2037 (0.2037) time: 9.9307 data: 6.8192\nEpoch [2] [84/85] eta: 0:00:02 loss: 1.4593 (1.4705) loss_classifier: 0.4419 (0.4465) loss_box_reg: 0.3458 (0.3467) loss_objectness: 0.4505 (0.4554) loss_rpn_box_reg: 0.2211 (0.2218) time: 1.9677 data: 0.1634\nEpoch [2] Total time: 0:03:15\nTest [0/21] eta: 0:01:48 model_time: 1.0188 (1.0188) evaluator_time: 0.0375 (0.0375) time: 5.1583 data: 4.0081\nTest [20/21] eta: 0:00:01 model_time: 0.7747 (0.7863) evaluator_time: 0.0245 (0.0251) time: 1.1125 data: 0.2520\nTest Total time: 0:00:27\nmAP @[IoU=0.50] = 0.7846\nmAP @[IoU=0.55] = 0.7775\nmAP @[IoU=0.60] = 0.7680\nmAP @[IoU=0.65] = 0.7548\nmAP @[IoU=0.70] = 0.7361\nmAP @[IoU=0.75] = 0.7068\nmAP @[IoU=0.50:0.75] = 0.7546\nValidation mAP increased (0.7159 --> 0.7546).  Saving model ...\nEpoch [3] [0/85] eta: 0:14:05 loss: 1.4574 (1.4574) loss_classifier: 0.4386 (0.4386) loss_box_reg: 0.3494 (0.3494) loss_objectness: 0.4494 (0.4494) loss_rpn_box_reg: 0.2200 (0.2200) time: 9.9514 data: 7.3169\nEpoch [3] [84/85] eta: 0:00:02 loss: 1.4105 (1.4320) loss_classifier: 0.4249 (0.4313) loss_box_reg: 0.3401 (0.3439) loss_objectness: 0.4309 (0.4390) loss_rpn_box_reg: 0.2146 (0.2178) time: 2.0205 data: 0.1838\nEpoch [3] Total time: 0:03:19\nTest [0/21] eta: 0:01:36 model_time: 1.0039 (1.0039) evaluator_time: 0.0339 (0.0339) time: 4.5980 data: 3.4248\nTest [20/21] eta: 0:00:01 model_time: 0.7993 (0.8090) evaluator_time: 0.0228 (0.0233) time: 1.1531 data: 0.2664\nTest Total time: 0:00:27\nmAP @[IoU=0.50] = 0.7861\nmAP @[IoU=0.55] = 0.7788\nmAP @[IoU=0.60] = 0.7694\nmAP @[IoU=0.65] = 0.7579\nmAP @[IoU=0.70] = 0.7406\nmAP @[IoU=0.75] = 0.7142\nmAP @[IoU=0.50:0.75] = 0.7578\nValidation mAP increased (0.7546 --> 0.7578).  Saving model ...\nEpoch [4] [0/85] eta: 0:13:38 loss: 1.4522 (1.4522) loss_classifier: 0.4365 (0.4365) loss_box_reg: 0.3328 (0.3328) loss_objectness: 0.4514 (0.4514) loss_rpn_box_reg: 0.2315 (0.2315) time: 9.6239 data: 6.7880\nEpoch [4] [84/85] eta: 0:00:02 loss: 1.4089 (1.4119) loss_classifier: 0.4215 (0.4248) loss_box_reg: 0.3404 (0.3412) loss_objectness: 0.4297 (0.4298) loss_rpn_box_reg: 0.2173 (0.2162) time: 2.0309 data: 0.1659\nEpoch [4] Total time: 0:03:17\nTest [0/21] eta: 0:02:07 model_time: 1.0625 (1.0625) evaluator_time: 0.0341 (0.0341) time: 6.0588 data: 4.8661\nTest [20/21] eta: 0:00:01 model_time: 0.7675 (0.7816) evaluator_time: 0.0245 (0.0249) time: 1.0422 data: 0.1843\nTest Total time: 0:00:26\nmAP @[IoU=0.50] = 0.8148\nmAP @[IoU=0.55] = 0.8097\nmAP @[IoU=0.60] = 0.8003\nmAP @[IoU=0.65] = 0.7893\nmAP @[IoU=0.70] = 0.7726\nmAP @[IoU=0.75] = 0.7420\nmAP @[IoU=0.50:0.75] = 0.7881\nValidation mAP increased (0.7578 --> 0.7881).  Saving model ...\nEpoch [5] [0/85] eta: 0:14:54 loss: 1.4061 (1.4061) loss_classifier: 0.4184 (0.4184) loss_box_reg: 0.3366 (0.3366) loss_objectness: 0.4332 (0.4332) loss_rpn_box_reg: 0.2179 (0.2179) time: 10.5181 data: 7.7419\nEpoch [5] [84/85] eta: 0:00:02 loss: 1.4101 (1.4047) loss_classifier: 0.4227 (0.4221) loss_box_reg: 0.3427 (0.3428) loss_objectness: 0.4288 (0.4242) loss_rpn_box_reg: 0.2160 (0.2156) time: 2.0179 data: 0.1754\nEpoch [5] Total time: 0:03:17\nTest [0/21] eta: 0:01:49 model_time: 1.0715 (1.0715) evaluator_time: 0.0425 (0.0425) time: 5.2132 data: 3.9985\nTest [20/21] eta: 0:00:01 model_time: 0.7900 (0.8034) evaluator_time: 0.0207 (0.0218) time: 1.0492 data: 0.1786\nTest Total time: 0:00:26\nmAP @[IoU=0.50] = 0.8110\nmAP @[IoU=0.55] = 0.8054\nmAP @[IoU=0.60] = 0.7960\nmAP @[IoU=0.65] = 0.7859\nmAP @[IoU=0.70] = 0.7688\nmAP @[IoU=0.75] = 0.7394\nmAP @[IoU=0.50:0.75] = 0.7844\nEarlyStopping counter: 1 out of 10\nEpoch [6] [0/85] eta: 0:12:58 loss: 1.4118 (1.4118) loss_classifier: 0.4337 (0.4337) loss_box_reg: 0.3642 (0.3642) loss_objectness: 0.4120 (0.4120) loss_rpn_box_reg: 0.2019 (0.2019) time: 9.1622 data: 6.5620\nEpoch [6] [84/85] eta: 0:00:02 loss: 1.3768 (1.3952) loss_classifier: 0.4140 (0.4165) loss_box_reg: 0.3406 (0.3412) loss_objectness: 0.4109 (0.4230) loss_rpn_box_reg: 0.2112 (0.2146) time: 1.9636 data: 0.1363\nEpoch [6] Total time: 0:03:15\nTest [0/21] eta: 0:01:46 model_time: 1.1855 (1.1855) evaluator_time: 0.0681 (0.0681) time: 5.0859 data: 3.7371\nTest [20/21] eta: 0:00:01 model_time: 0.7679 (0.7878) evaluator_time: 0.0205 (0.0228) time: 1.0659 data: 0.2134\nTest Total time: 0:00:26\nmAP @[IoU=0.50] = 0.8245\nmAP @[IoU=0.55] = 0.8175\nmAP @[IoU=0.60] = 0.8093\nmAP @[IoU=0.65] = 0.7959\nmAP @[IoU=0.70] = 0.7776\nmAP @[IoU=0.75] = 0.7448\nmAP @[IoU=0.50:0.75] = 0.7949\nValidation mAP increased (0.7881 --> 0.7949).  Saving model ...\nEpoch [7] [0/85] eta: 0:12:56 loss: 1.3562 (1.3562) loss_classifier: 0.4005 (0.4005) loss_box_reg: 0.3317 (0.3317) loss_objectness: 0.4034 (0.4034) loss_rpn_box_reg: 0.2207 (0.2207) time: 9.1346 data: 6.4358\nEpoch [7] [84/85] eta: 0:00:02 loss: 1.3756 (1.3758) loss_classifier: 0.4088 (0.4123) loss_box_reg: 0.3378 (0.3395) loss_objectness: 0.4120 (0.4115) loss_rpn_box_reg: 0.2171 (0.2125) time: 1.9742 data: 0.1786\nEpoch [7] Total time: 0:03:15\nTest [0/21] eta: 0:01:59 model_time: 0.9880 (0.9880) evaluator_time: 0.0477 (0.0477) time: 5.6781 data: 4.5690\nTest [20/21] eta: 0:00:01 model_time: 0.8198 (0.8278) evaluator_time: 0.0200 (0.0213) time: 1.0842 data: 0.1869\nTest Total time: 0:00:27\nmAP @[IoU=0.50] = 0.8243\nmAP @[IoU=0.55] = 0.8181\nmAP @[IoU=0.60] = 0.8096\nmAP @[IoU=0.65] = 0.7988\nmAP @[IoU=0.70] = 0.7823\nmAP @[IoU=0.75] = 0.7541\nmAP @[IoU=0.50:0.75] = 0.7979\nValidation mAP increased (0.7949 --> 0.7979).  Saving model ...\nEpoch [8] [0/85] eta: 0:13:57 loss: 1.4502 (1.4502) loss_classifier: 0.4198 (0.4198) loss_box_reg: 0.3707 (0.3707) loss_objectness: 0.4343 (0.4343) loss_rpn_box_reg: 0.2254 (0.2254) time: 9.8506 data: 6.9577\nEpoch [8] [84/85] eta: 0:00:02 loss: 1.3722 (1.3720) loss_classifier: 0.4119 (0.4110) loss_box_reg: 0.3387 (0.3387) loss_objectness: 0.4103 (0.4105) loss_rpn_box_reg: 0.2113 (0.2118) time: 2.0319 data: 0.1977\nEpoch [8] Total time: 0:03:18\nTest [0/21] eta: 0:01:56 model_time: 1.0613 (1.0613) evaluator_time: 0.0573 (0.0573) time: 5.5547 data: 4.3464\nTest [20/21] eta: 0:00:01 model_time: 0.7813 (0.7947) evaluator_time: 0.0191 (0.0209) time: 1.0470 data: 0.1805\nTest Total time: 0:00:26\nmAP @[IoU=0.50] = 0.8277\nmAP @[IoU=0.55] = 0.8211\nmAP @[IoU=0.60] = 0.8123\nmAP @[IoU=0.65] = 0.8017\nmAP @[IoU=0.70] = 0.7862\nmAP @[IoU=0.75] = 0.7588\nmAP @[IoU=0.50:0.75] = 0.8013\nValidation mAP increased (0.7979 --> 0.8013).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Epoch [9] [0/85] eta: 0:14:35 loss: 1.5066 (1.5066) loss_classifier: 0.4533 (0.4533) loss_box_reg: 0.3611 (0.3611) loss_objectness: 0.4622 (0.4622) loss_rpn_box_reg: 0.2301 (0.2301) time: 10.2949 data: 7.6599\nEpoch [9] [84/85] eta: 0:00:02 loss: 1.3620 (1.3665) loss_classifier: 0.4077 (0.4073) loss_box_reg: 0.3382 (0.3385) loss_objectness: 0.4056 (0.4087) loss_rpn_box_reg: 0.2105 (0.2119) time: 2.0139 data: 0.1889\nEpoch [9] Total time: 0:03:18\nTest [0/21] eta: 0:01:38 model_time: 0.8069 (0.8069) evaluator_time: 0.0146 (0.0146) time: 4.7063 data: 3.7922\nTest [20/21] eta: 0:00:01 model_time: 0.7921 (0.7928) evaluator_time: 0.0228 (0.0224) time: 1.1100 data: 0.2369\nTest Total time: 0:00:26\nmAP @[IoU=0.50] = 0.8323\nmAP @[IoU=0.55] = 0.8260\nmAP @[IoU=0.60] = 0.8182\nmAP @[IoU=0.65] = 0.8059\nmAP @[IoU=0.70] = 0.7908\nmAP @[IoU=0.75] = 0.7642\nmAP @[IoU=0.50:0.75] = 0.8062\nValidation mAP increased (0.8013 --> 0.8062).  Saving model ...\nEpoch [10] [0/85] eta: 0:13:54 loss: 1.4764 (1.4764) loss_classifier: 0.4216 (0.4216) loss_box_reg: 0.3461 (0.3461) loss_objectness: 0.4740 (0.4740) loss_rpn_box_reg: 0.2347 (0.2347) time: 9.8235 data: 7.0751\nEpoch [10] [84/85] eta: 0:00:02 loss: 1.3735 (1.3670) loss_classifier: 0.4066 (0.4072) loss_box_reg: 0.3399 (0.3386) loss_objectness: 0.4123 (0.4091) loss_rpn_box_reg: 0.2147 (0.2121) time: 2.0116 data: 0.1822\nEpoch [10] Total time: 0:03:18\nTest [0/21] eta: 0:01:56 model_time: 1.0607 (1.0607) evaluator_time: 0.0459 (0.0459) time: 5.5352 data: 4.3127\nTest [20/21] eta: 0:00:01 model_time: 0.7799 (0.7933) evaluator_time: 0.0196 (0.0208) time: 1.0488 data: 0.1875\nTest Total time: 0:00:26\nmAP @[IoU=0.50] = 0.8238\nmAP @[IoU=0.55] = 0.8181\nmAP @[IoU=0.60] = 0.8096\nmAP @[IoU=0.65] = 0.7987\nmAP @[IoU=0.70] = 0.7834\nmAP @[IoU=0.75] = 0.7574\nmAP @[IoU=0.50:0.75] = 0.7985\nEarlyStopping counter: 1 out of 10\nEpoch [11] [0/85] eta: 0:14:02 loss: 1.4307 (1.4307) loss_classifier: 0.4278 (0.4278) loss_box_reg: 0.3588 (0.3588) loss_objectness: 0.4276 (0.4276) loss_rpn_box_reg: 0.2165 (0.2165) time: 9.9173 data: 7.4363\nEpoch [11] [84/85] eta: 0:00:02 loss: 1.3199 (1.3439) loss_classifier: 0.3983 (0.4023) loss_box_reg: 0.3340 (0.3359) loss_objectness: 0.3842 (0.3969) loss_rpn_box_reg: 0.2034 (0.2089) time: 2.0039 data: 0.1857\nEpoch [11] Total time: 0:03:17\nTest [0/21] eta: 0:02:06 model_time: 1.1880 (1.1880) evaluator_time: 0.0153 (0.0153) time: 6.0161 data: 4.6910\nTest [20/21] eta: 0:00:01 model_time: 0.8140 (0.8318) evaluator_time: 0.0211 (0.0208) time: 1.0754 data: 0.1830\nTest Total time: 0:00:27\nmAP @[IoU=0.50] = 0.8318\nmAP @[IoU=0.55] = 0.8251\nmAP @[IoU=0.60] = 0.8174\nmAP @[IoU=0.65] = 0.8072\nmAP @[IoU=0.70] = 0.7910\nmAP @[IoU=0.75] = 0.7643\nmAP @[IoU=0.50:0.75] = 0.8061\nEarlyStopping counter: 2 out of 10\nEpoch [12] [0/85] eta: 0:14:01 loss: 1.2870 (1.2870) loss_classifier: 0.3824 (0.3824) loss_box_reg: 0.3276 (0.3276) loss_objectness: 0.3691 (0.3691) loss_rpn_box_reg: 0.2078 (0.2078) time: 9.9036 data: 7.0044\nEpoch [12] [84/85] eta: 0:00:02 loss: 1.3542 (1.3526) loss_classifier: 0.4006 (0.4036) loss_box_reg: 0.3310 (0.3372) loss_objectness: 0.4115 (0.4016) loss_rpn_box_reg: 0.2111 (0.2102) time: 2.0287 data: 0.1925\nEpoch [12] Total time: 0:03:18\nTest [0/21] eta: 0:02:00 model_time: 1.0434 (1.0434) evaluator_time: 0.0684 (0.0684) time: 5.7403 data: 4.5274\nTest [20/21] eta: 0:00:01 model_time: 0.7940 (0.8058) evaluator_time: 0.0195 (0.0218) time: 1.0499 data: 0.1780\nTest Total time: 0:00:26\nmAP @[IoU=0.50] = 0.8333\nmAP @[IoU=0.55] = 0.8279\nmAP @[IoU=0.60] = 0.8203\nmAP @[IoU=0.65] = 0.8104\nmAP @[IoU=0.70] = 0.7934\nmAP @[IoU=0.75] = 0.7679\nmAP @[IoU=0.50:0.75] = 0.8089\nValidation mAP increased (0.8062 --> 0.8089).  Saving model ...\nEpoch [13] [0/85] eta: 0:14:14 loss: 1.3343 (1.3343) loss_classifier: 0.4173 (0.4173) loss_box_reg: 0.3235 (0.3235) loss_objectness: 0.4076 (0.4076) loss_rpn_box_reg: 0.1858 (0.1858) time: 10.0529 data: 7.3827\nEpoch [13] [84/85] eta: 0:00:02 loss: 1.3488 (1.3374) loss_classifier: 0.4065 (0.3992) loss_box_reg: 0.3349 (0.3336) loss_objectness: 0.4017 (0.3962) loss_rpn_box_reg: 0.2057 (0.2084) time: 2.0386 data: 0.1836\nEpoch [13] Total time: 0:03:18\nTest [0/21] eta: 0:02:02 model_time: 1.0705 (1.0705) evaluator_time: 0.0282 (0.0282) time: 5.8172 data: 4.6389\nTest [20/21] eta: 0:00:01 model_time: 0.7624 (0.7771) evaluator_time: 0.0172 (0.0177) time: 1.0297 data: 0.1847\nTest Total time: 0:00:26\nmAP @[IoU=0.50] = 0.8308\nmAP @[IoU=0.55] = 0.8253\nmAP @[IoU=0.60] = 0.8173\nmAP @[IoU=0.65] = 0.8070\nmAP @[IoU=0.70] = 0.7911\nmAP @[IoU=0.75] = 0.7665\nmAP @[IoU=0.50:0.75] = 0.8063\nEarlyStopping counter: 1 out of 10\nEpoch [14] [0/85] eta: 0:14:07 loss: 1.4218 (1.4218) loss_classifier: 0.4045 (0.4045) loss_box_reg: 0.3582 (0.3582) loss_objectness: 0.4220 (0.4220) loss_rpn_box_reg: 0.2371 (0.2371) time: 9.9736 data: 7.2239\nEpoch [14] [84/85] eta: 0:00:02 loss: 1.3261 (1.3329) loss_classifier: 0.3939 (0.3981) loss_box_reg: 0.3345 (0.3343) loss_objectness: 0.3878 (0.3918) loss_rpn_box_reg: 0.2099 (0.2087) time: 2.0417 data: 0.2018\nEpoch [14] Total time: 0:03:17\nTest [0/21] eta: 0:01:51 model_time: 0.9915 (0.9915) evaluator_time: 0.0475 (0.0475) time: 5.2900 data: 4.1534\nTest [20/21] eta: 0:00:01 model_time: 0.7598 (0.7709) evaluator_time: 0.0214 (0.0226) time: 1.0437 data: 0.2017\nTest Total time: 0:00:26\nmAP @[IoU=0.50] = 0.8354\nmAP @[IoU=0.55] = 0.8297\nmAP @[IoU=0.60] = 0.8211\nmAP @[IoU=0.65] = 0.8097\nmAP @[IoU=0.70] = 0.7951\nmAP @[IoU=0.75] = 0.7700\nmAP @[IoU=0.50:0.75] = 0.8102\nValidation mAP increased (0.8089 --> 0.8102).  Saving model ...\nEpoch [15] [0/85] eta: 0:12:49 loss: 1.2634 (1.2634) loss_classifier: 0.3874 (0.3874) loss_box_reg: 0.3303 (0.3303) loss_objectness: 0.3480 (0.3480) loss_rpn_box_reg: 0.1978 (0.1978) time: 9.0487 data: 6.4324\nEpoch [15] [84/85] eta: 0:00:02 loss: 1.3466 (1.3367) loss_classifier: 0.4011 (0.3982) loss_box_reg: 0.3348 (0.3353) loss_objectness: 0.3980 (0.3943) loss_rpn_box_reg: 0.2127 (0.2090) time: 1.9934 data: 0.1904\nEpoch [15] Total time: 0:03:18\nTest [0/21] eta: 0:01:46 model_time: 1.1664 (1.1664) evaluator_time: 0.0648 (0.0648) time: 5.0701 data: 3.7531\nTest [20/21] eta: 0:00:01 model_time: 0.8034 (0.8207) evaluator_time: 0.0208 (0.0229) time: 1.1160 data: 0.2200\nTest Total time: 0:00:27\nmAP @[IoU=0.50] = 0.8380\nmAP @[IoU=0.55] = 0.8327\nmAP @[IoU=0.60] = 0.8249\nmAP @[IoU=0.65] = 0.8144\nmAP @[IoU=0.70] = 0.7996\nmAP @[IoU=0.75] = 0.7746\nmAP @[IoU=0.50:0.75] = 0.8140\nValidation mAP increased (0.8102 --> 0.8140).  Saving model ...\nEpoch [16] [0/85] eta: 0:12:45 loss: 1.3068 (1.3068) loss_classifier: 0.3925 (0.3925) loss_box_reg: 0.3082 (0.3082) loss_objectness: 0.4048 (0.4048) loss_rpn_box_reg: 0.2012 (0.2012) time: 9.0090 data: 6.3708\nEpoch [16] [84/85] eta: 0:00:02 loss: 1.3311 (1.3273) loss_classifier: 0.3976 (0.3969) loss_box_reg: 0.3363 (0.3356) loss_objectness: 0.3911 (0.3874) loss_rpn_box_reg: 0.2062 (0.2073) time: 1.9980 data: 0.1820\nEpoch [16] Total time: 0:03:16\nTest [0/21] eta: 0:01:56 model_time: 1.0372 (1.0372) evaluator_time: 0.0280 (0.0280) time: 5.5322 data: 4.3781\nTest [20/21] eta: 0:00:01 model_time: 0.7904 (0.8021) evaluator_time: 0.0184 (0.0188) time: 1.0359 data: 0.1657\nTest Total time: 0:00:26\nmAP @[IoU=0.50] = 0.8438\nmAP @[IoU=0.55] = 0.8385\nmAP @[IoU=0.60] = 0.8303\nmAP @[IoU=0.65] = 0.8191\nmAP @[IoU=0.70] = 0.8051\nmAP @[IoU=0.75] = 0.7810\nmAP @[IoU=0.50:0.75] = 0.8196\nValidation mAP increased (0.8140 --> 0.8196).  Saving model ...\nEpoch [17] [0/85] eta: 0:13:49 loss: 1.3842 (1.3842) loss_classifier: 0.4103 (0.4103) loss_box_reg: 0.3397 (0.3397) loss_objectness: 0.4178 (0.4178) loss_rpn_box_reg: 0.2165 (0.2165) time: 9.7632 data: 6.8600\nEpoch [17] [84/85] eta: 0:00:02 loss: 1.3174 (1.3244) loss_classifier: 0.3921 (0.3946) loss_box_reg: 0.3367 (0.3360) loss_objectness: 0.3816 (0.3872) loss_rpn_box_reg: 0.2071 (0.2067) time: 2.0543 data: 0.1951\nEpoch [17] Total time: 0:03:18\nTest [0/21] eta: 0:01:42 model_time: 1.0213 (1.0213) evaluator_time: 0.0563 (0.0563) time: 4.8781 data: 3.7221\nTest [20/21] eta: 0:00:01 model_time: 0.7752 (0.7869) evaluator_time: 0.0219 (0.0235) time: 1.0687 data: 0.2047\nTest Total time: 0:00:26\nmAP @[IoU=0.50] = 0.8446\nmAP @[IoU=0.55] = 0.8387\nmAP @[IoU=0.60] = 0.8305\nmAP @[IoU=0.65] = 0.8202\nmAP @[IoU=0.70] = 0.8077\nmAP @[IoU=0.75] = 0.7813\nmAP @[IoU=0.50:0.75] = 0.8205\nValidation mAP increased (0.8196 --> 0.8205).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Epoch [18] [0/85] eta: 0:13:56 loss: 1.3228 (1.3228) loss_classifier: 0.3972 (0.3972) loss_box_reg: 0.3518 (0.3518) loss_objectness: 0.3692 (0.3692) loss_rpn_box_reg: 0.2047 (0.2047) time: 9.8426 data: 7.1647\nEpoch [18] [84/85] eta: 0:00:02 loss: 1.2916 (1.3140) loss_classifier: 0.3834 (0.3911) loss_box_reg: 0.3267 (0.3316) loss_objectness: 0.3767 (0.3852) loss_rpn_box_reg: 0.2048 (0.2060) time: 1.9610 data: 0.1587\nEpoch [18] Total time: 0:03:17\nTest [0/21] eta: 0:02:12 model_time: 1.0595 (1.0595) evaluator_time: 0.0612 (0.0612) time: 6.3272 data: 5.1299\nTest [20/21] eta: 0:00:01 model_time: 0.7722 (0.7859) evaluator_time: 0.0197 (0.0216) time: 1.0495 data: 0.1925\nTest Total time: 0:00:27\nmAP @[IoU=0.50] = 0.8473\nmAP @[IoU=0.55] = 0.8410\nmAP @[IoU=0.60] = 0.8337\nmAP @[IoU=0.65] = 0.8220\nmAP @[IoU=0.70] = 0.8067\nmAP @[IoU=0.75] = 0.7818\nmAP @[IoU=0.50:0.75] = 0.8221\nValidation mAP increased (0.8205 --> 0.8221).  Saving model ...\nEpoch [19] [0/85] eta: 0:14:12 loss: 1.3666 (1.3666) loss_classifier: 0.3995 (0.3995) loss_box_reg: 0.3416 (0.3416) loss_objectness: 0.4068 (0.4068) loss_rpn_box_reg: 0.2186 (0.2186) time: 10.0282 data: 7.2608\nEpoch [19] [84/85] eta: 0:00:02 loss: 1.2884 (1.3123) loss_classifier: 0.3827 (0.3892) loss_box_reg: 0.3217 (0.3314) loss_objectness: 0.3792 (0.3853) loss_rpn_box_reg: 0.2049 (0.2063) time: 2.0047 data: 0.1797\nEpoch [19] Total time: 0:03:17\nTest [0/21] eta: 0:01:37 model_time: 1.0197 (1.0197) evaluator_time: 0.0298 (0.0298) time: 4.6298 data: 3.4717\nTest [20/21] eta: 0:00:01 model_time: 0.8012 (0.8116) evaluator_time: 0.0245 (0.0248) time: 1.1172 data: 0.2154\nTest Total time: 0:00:27\nmAP @[IoU=0.50] = 0.8494\nmAP @[IoU=0.55] = 0.8448\nmAP @[IoU=0.60] = 0.8362\nmAP @[IoU=0.65] = 0.8273\nmAP @[IoU=0.70] = 0.8112\nmAP @[IoU=0.75] = 0.7855\nmAP @[IoU=0.50:0.75] = 0.8257\nValidation mAP increased (0.8221 --> 0.8257).  Saving model ...\nEpoch [20] [0/85] eta: 0:14:26 loss: 1.3541 (1.3541) loss_classifier: 0.3875 (0.3875) loss_box_reg: 0.3263 (0.3263) loss_objectness: 0.4160 (0.4160) loss_rpn_box_reg: 0.2243 (0.2243) time: 10.1885 data: 7.4003\nEpoch [20] [84/85] eta: 0:00:02 loss: 1.3154 (1.3125) loss_classifier: 0.3960 (0.3912) loss_box_reg: 0.3401 (0.3320) loss_objectness: 0.3774 (0.3838) loss_rpn_box_reg: 0.2020 (0.2055) time: 2.0481 data: 0.1719\nEpoch [20] Total time: 0:03:19\nTest [0/21] eta: 0:01:59 model_time: 1.0477 (1.0477) evaluator_time: 0.0300 (0.0300) time: 5.6938 data: 4.5421\nTest [20/21] eta: 0:00:01 model_time: 0.7807 (0.7934) evaluator_time: 0.0196 (0.0201) time: 1.0496 data: 0.1816\nTest Total time: 0:00:26\nmAP @[IoU=0.50] = 0.8485\nmAP @[IoU=0.55] = 0.8440\nmAP @[IoU=0.60] = 0.8364\nmAP @[IoU=0.65] = 0.8269\nmAP @[IoU=0.70] = 0.8110\nmAP @[IoU=0.75] = 0.7869\nmAP @[IoU=0.50:0.75] = 0.8256\nEarlyStopping counter: 1 out of 10\nEpoch [21] [0/85] eta: 0:14:48 loss: 1.3664 (1.3664) loss_classifier: 0.4000 (0.4000) loss_box_reg: 0.3320 (0.3320) loss_objectness: 0.4090 (0.4090) loss_rpn_box_reg: 0.2254 (0.2254) time: 10.4541 data: 7.7259\nEpoch [21] [84/85] eta: 0:00:02 loss: 1.2936 (1.3093) loss_classifier: 0.3891 (0.3905) loss_box_reg: 0.3323 (0.3315) loss_objectness: 0.3692 (0.3815) loss_rpn_box_reg: 0.2030 (0.2057) time: 2.0300 data: 0.1762\nEpoch [21] Total time: 0:03:18\nTest [0/21] eta: 0:01:42 model_time: 0.9907 (0.9907) evaluator_time: 0.0489 (0.0489) time: 4.8882 data: 3.7530\nTest [20/21] eta: 0:00:01 model_time: 0.7819 (0.7919) evaluator_time: 0.0186 (0.0200) time: 1.0707 data: 0.2051\nTest Total time: 0:00:26\nmAP @[IoU=0.50] = 0.8476\nmAP @[IoU=0.55] = 0.8409\nmAP @[IoU=0.60] = 0.8326\nmAP @[IoU=0.65] = 0.8229\nmAP @[IoU=0.70] = 0.8085\nmAP @[IoU=0.75] = 0.7825\nmAP @[IoU=0.50:0.75] = 0.8225\nEarlyStopping counter: 2 out of 10\nEpoch [22] [0/85] eta: 0:13:01 loss: 1.3178 (1.3178) loss_classifier: 0.3885 (0.3885) loss_box_reg: 0.3231 (0.3231) loss_objectness: 0.4037 (0.4037) loss_rpn_box_reg: 0.2025 (0.2025) time: 9.1964 data: 6.4076\nEpoch [22] [84/85] eta: 0:00:02 loss: 1.2956 (1.3102) loss_classifier: 0.3894 (0.3896) loss_box_reg: 0.3325 (0.3310) loss_objectness: 0.3737 (0.3830) loss_rpn_box_reg: 0.2000 (0.2066) time: 1.9809 data: 0.1820\nEpoch [22] Total time: 0:03:18\nTest [0/21] eta: 0:01:47 model_time: 0.9895 (0.9895) evaluator_time: 0.0412 (0.0412) time: 5.1199 data: 4.0005\nTest [20/21] eta: 0:00:01 model_time: 0.7869 (0.7966) evaluator_time: 0.0215 (0.0225) time: 1.0800 data: 0.2046\nTest Total time: 0:00:26\nmAP @[IoU=0.50] = 0.8517\nmAP @[IoU=0.55] = 0.8470\nmAP @[IoU=0.60] = 0.8395\nmAP @[IoU=0.65] = 0.8298\nmAP @[IoU=0.70] = 0.8143\nmAP @[IoU=0.75] = 0.7911\nmAP @[IoU=0.50:0.75] = 0.8289\nValidation mAP increased (0.8257 --> 0.8289).  Saving model ...\nEpoch [23] [0/85] eta: 0:12:57 loss: 1.2662 (1.2662) loss_classifier: 0.3563 (0.3563) loss_box_reg: 0.3110 (0.3110) loss_objectness: 0.3862 (0.3862) loss_rpn_box_reg: 0.2127 (0.2127) time: 9.1492 data: 6.1236\nEpoch [23] [84/85] eta: 0:00:02 loss: 1.2752 (1.2952) loss_classifier: 0.3785 (0.3846) loss_box_reg: 0.3278 (0.3310) loss_objectness: 0.3664 (0.3751) loss_rpn_box_reg: 0.2026 (0.2045) time: 1.9933 data: 0.1801\nEpoch [23] Total time: 0:03:17\nTest [0/21] eta: 0:01:50 model_time: 0.9490 (0.9490) evaluator_time: 0.0670 (0.0670) time: 5.2382 data: 4.1015\nTest [20/21] eta: 0:00:01 model_time: 0.7865 (0.7942) evaluator_time: 0.0267 (0.0287) time: 1.1058 data: 0.2346\nTest Total time: 0:00:27\nmAP @[IoU=0.50] = 0.8418\nmAP @[IoU=0.55] = 0.8369\nmAP @[IoU=0.60] = 0.8289\nmAP @[IoU=0.65] = 0.8179\nmAP @[IoU=0.70] = 0.8016\nmAP @[IoU=0.75] = 0.7745\nmAP @[IoU=0.50:0.75] = 0.8169\nEarlyStopping counter: 1 out of 10\nEpoch [24] [0/85] eta: 0:12:24 loss: 1.3171 (1.3171) loss_classifier: 0.3855 (0.3855) loss_box_reg: 0.3384 (0.3384) loss_objectness: 0.3899 (0.3899) loss_rpn_box_reg: 0.2033 (0.2033) time: 8.7599 data: 6.0821\nEpoch [24] [84/85] eta: 0:00:02 loss: 1.2868 (1.2965) loss_classifier: 0.3800 (0.3838) loss_box_reg: 0.3248 (0.3271) loss_objectness: 0.3791 (0.3806) loss_rpn_box_reg: 0.2028 (0.2050) time: 2.0591 data: 0.2001\nEpoch [24] Total time: 0:03:18\nTest [0/21] eta: 0:01:42 model_time: 1.1052 (1.1052) evaluator_time: 0.0525 (0.0525) time: 4.8961 data: 3.5916\nTest [20/21] eta: 0:00:01 model_time: 0.7722 (0.7880) evaluator_time: 0.0168 (0.0185) time: 1.0975 data: 0.2502\nTest Total time: 0:00:26\nmAP @[IoU=0.50] = 0.8510\nmAP @[IoU=0.55] = 0.8452\nmAP @[IoU=0.60] = 0.8393\nmAP @[IoU=0.65] = 0.8292\nmAP @[IoU=0.70] = 0.8138\nmAP @[IoU=0.75] = 0.7895\nmAP @[IoU=0.50:0.75] = 0.8280\nEarlyStopping counter: 2 out of 10\nEpoch [25] [0/85] eta: 0:14:52 loss: 1.2356 (1.2356) loss_classifier: 0.3835 (0.3835) loss_box_reg: 0.3273 (0.3273) loss_objectness: 0.3367 (0.3367) loss_rpn_box_reg: 0.1881 (0.1881) time: 10.4966 data: 7.5939\nEpoch [25] [84/85] eta: 0:00:02 loss: 1.2844 (1.2844) loss_classifier: 0.3834 (0.3833) loss_box_reg: 0.3307 (0.3293) loss_objectness: 0.3687 (0.3691) loss_rpn_box_reg: 0.2015 (0.2028) time: 2.0332 data: 0.1840\nEpoch [25] Total time: 0:03:19\nTest [0/21] eta: 0:01:45 model_time: 0.9840 (0.9840) evaluator_time: 0.0707 (0.0707) time: 5.0073 data: 3.8598\nTest [20/21] eta: 0:00:01 model_time: 0.7718 (0.7819) evaluator_time: 0.0210 (0.0234) time: 1.0576 data: 0.1985\nTest Total time: 0:00:26\nmAP @[IoU=0.50] = 0.8474\nmAP @[IoU=0.55] = 0.8424\nmAP @[IoU=0.60] = 0.8361\nmAP @[IoU=0.65] = 0.8256\nmAP @[IoU=0.70] = 0.8107\nmAP @[IoU=0.75] = 0.7868\nmAP @[IoU=0.50:0.75] = 0.8248\nEarlyStopping counter: 3 out of 10\nEpoch [26] [0/85] eta: 0:14:01 loss: 1.4209 (1.4209) loss_classifier: 0.3911 (0.3911) loss_box_reg: 0.3274 (0.3274) loss_objectness: 0.4642 (0.4642) loss_rpn_box_reg: 0.2381 (0.2381) time: 9.8959 data: 7.2198\nEpoch [26] [84/85] eta: 0:00:02 loss: 1.3078 (1.2944) loss_classifier: 0.3858 (0.3843) loss_box_reg: 0.3315 (0.3287) loss_objectness: 0.3834 (0.3773) loss_rpn_box_reg: 0.2071 (0.2041) time: 2.0103 data: 0.1839\nEpoch [26] Total time: 0:03:18\nTest [0/21] eta: 0:01:52 model_time: 1.0897 (1.0897) evaluator_time: 0.0463 (0.0463) time: 5.3794 data: 4.1157\nTest [20/21] eta: 0:00:01 model_time: 0.7784 (0.7933) evaluator_time: 0.0199 (0.0211) time: 1.0583 data: 0.2026\nTest Total time: 0:00:26\nmAP @[IoU=0.50] = 0.8589\nmAP @[IoU=0.55] = 0.8543\nmAP @[IoU=0.60] = 0.8473\nmAP @[IoU=0.65] = 0.8380\nmAP @[IoU=0.70] = 0.8237\nmAP @[IoU=0.75] = 0.7993\nmAP @[IoU=0.50:0.75] = 0.8369\nValidation mAP increased (0.8289 --> 0.8369).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Epoch [27] [0/85] eta: 0:13:21 loss: 1.3782 (1.3782) loss_classifier: 0.4165 (0.4165) loss_box_reg: 0.3496 (0.3496) loss_objectness: 0.4061 (0.4061) loss_rpn_box_reg: 0.2060 (0.2060) time: 9.4252 data: 6.5190\nEpoch [27] [84/85] eta: 0:00:02 loss: 1.2840 (1.2808) loss_classifier: 0.3804 (0.3797) loss_box_reg: 0.3223 (0.3260) loss_objectness: 0.3798 (0.3719) loss_rpn_box_reg: 0.2015 (0.2032) time: 1.9931 data: 0.1819\nEpoch [27] Total time: 0:03:17\nTest [0/21] eta: 0:01:59 model_time: 1.0014 (1.0014) evaluator_time: 0.0480 (0.0480) time: 5.6925 data: 4.5556\nTest [20/21] eta: 0:00:01 model_time: 0.8027 (0.8121) evaluator_time: 0.0229 (0.0241) time: 1.0880 data: 0.1962\nTest Total time: 0:00:27\nmAP @[IoU=0.50] = 0.8559\nmAP @[IoU=0.55] = 0.8516\nmAP @[IoU=0.60] = 0.8457\nmAP @[IoU=0.65] = 0.8326\nmAP @[IoU=0.70] = 0.8196\nmAP @[IoU=0.75] = 0.7918\nmAP @[IoU=0.50:0.75] = 0.8329\nEarlyStopping counter: 1 out of 10\nEpoch [28] [0/85] eta: 0:14:07 loss: 1.2108 (1.2108) loss_classifier: 0.3658 (0.3658) loss_box_reg: 0.2962 (0.2962) loss_objectness: 0.3639 (0.3639) loss_rpn_box_reg: 0.1850 (0.1850) time: 9.9762 data: 7.3321\nEpoch [28] [84/85] eta: 0:00:02 loss: 1.2902 (1.2882) loss_classifier: 0.3827 (0.3822) loss_box_reg: 0.3293 (0.3287) loss_objectness: 0.3723 (0.3739) loss_rpn_box_reg: 0.2060 (0.2035) time: 2.0433 data: 0.1685\nEpoch [28] Total time: 0:03:17\nTest [0/21] eta: 0:01:39 model_time: 1.0540 (1.0540) evaluator_time: 0.0423 (0.0423) time: 4.7180 data: 3.4606\nTest [20/21] eta: 0:00:01 model_time: 0.7782 (0.7913) evaluator_time: 0.0203 (0.0213) time: 1.1106 data: 0.2490\nTest Total time: 0:00:26\nmAP @[IoU=0.50] = 0.8539\nmAP @[IoU=0.55] = 0.8488\nmAP @[IoU=0.60] = 0.8416\nmAP @[IoU=0.65] = 0.8311\nmAP @[IoU=0.70] = 0.8148\nmAP @[IoU=0.75] = 0.7928\nmAP @[IoU=0.50:0.75] = 0.8305\nEarlyStopping counter: 2 out of 10\nEpoch [29] [0/85] eta: 0:13:17 loss: 1.1663 (1.1663) loss_classifier: 0.3637 (0.3637) loss_box_reg: 0.3194 (0.3194) loss_objectness: 0.2985 (0.2985) loss_rpn_box_reg: 0.1847 (0.1847) time: 9.3874 data: 6.7309\nEpoch [29] [84/85] eta: 0:00:02 loss: 1.2671 (1.2784) loss_classifier: 0.3809 (0.3798) loss_box_reg: 0.3251 (0.3253) loss_objectness: 0.3636 (0.3712) loss_rpn_box_reg: 0.1975 (0.2021) time: 2.0465 data: 0.1843\nEpoch [29] Total time: 0:03:17\nTest [0/21] eta: 0:01:43 model_time: 1.0317 (1.0317) evaluator_time: 0.0361 (0.0361) time: 4.9080 data: 3.7564\nTest [20/21] eta: 0:00:01 model_time: 0.7651 (0.7778) evaluator_time: 0.0193 (0.0201) time: 1.0500 data: 0.2053\nTest Total time: 0:00:25\nmAP @[IoU=0.50] = 0.8581\nmAP @[IoU=0.55] = 0.8533\nmAP @[IoU=0.60] = 0.8470\nmAP @[IoU=0.65] = 0.8376\nmAP @[IoU=0.70] = 0.8235\nmAP @[IoU=0.75] = 0.7996\nmAP @[IoU=0.50:0.75] = 0.8365\nEarlyStopping counter: 3 out of 10\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#torch.save(model.state_dict(), 'fasterrcnn_resnet50_fpn.pth')","execution_count":29,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False,\\\n                                                             pretrained_backbone=False,\\\n                                                             min_size=512, max_size=1024)\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, 2)\n\n# Load the trained weights\nWEIGHTS_FILE = '../input/ghdoutputv8/fasterrcnn_resnet50_fpn.pth'\nmodel.load_state_dict(torch.load(WEIGHTS_FILE, map_location=torch.device(device)))\n'''","execution_count":30,"outputs":[{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"\"\\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False,                                                             pretrained_backbone=False,                                                             min_size=512, max_size=1024)\\nin_features = model.roi_heads.box_predictor.cls_score.in_features\\n\\n# replace the pre-trained head with a new one\\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, 2)\\n\\n# Load the trained weights\\nWEIGHTS_FILE = '../input/ghdoutputv8/fasterrcnn_resnet50_fpn.pth'\\nmodel.load_state_dict(torch.load(WEIGHTS_FILE, map_location=torch.device(device)))\\n\""},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''root = '/kaggle/input/global-wheat-detection/'\nimgs = os.listdir(root + 'train')'''","execution_count":31,"outputs":[{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"\"root = '/kaggle/input/global-wheat-detection/'\\nimgs = os.listdir(root + 'train')\""},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''def get_prediction(img, th=None):\n    \n    if isinstance(img, str):\n        img = get_img(img)\n        img = img.astype(np.float32)\n        img = img / 255.0\n        img = torch.from_numpy(img)\n        img = img.permute(2, 0, 1)\n    \n    output = model([img])\n    \n    with torch.no_grad():\n        bboxes = output[0]['boxes'].detach().numpy().astype(np.int32)\n        score = output[0]['scores'].detach().numpy()\n        \n        bboxes[:, 2] = bboxes[:, 2] - bboxes[:, 0]\n        bboxes[:, 3] = bboxes[:, 3] - bboxes[:, 1]\n        \n        if th:\n            qry = score >= th\n            bboxes = bboxes[qry]\n            score = score[qry]\n            print(score.shape, bboxes.shape)\n        \n    return bboxes, score ''' ","execution_count":32,"outputs":[{"output_type":"execute_result","execution_count":32,"data":{"text/plain":"\"def get_prediction(img, th=None):\\n    \\n    if isinstance(img, str):\\n        img = get_img(img)\\n        img = img.astype(np.float32)\\n        img = img / 255.0\\n        img = torch.from_numpy(img)\\n        img = img.permute(2, 0, 1)\\n    \\n    output = model([img])\\n    \\n    with torch.no_grad():\\n        bboxes = output[0]['boxes'].detach().numpy().astype(np.int32)\\n        score = output[0]['scores'].detach().numpy()\\n        \\n        bboxes[:, 2] = bboxes[:, 2] - bboxes[:, 0]\\n        bboxes[:, 3] = bboxes[:, 3] - bboxes[:, 1]\\n        \\n        if th:\\n            qry = score >= th\\n            bboxes = bboxes[qry]\\n            score = score[qry]\\n            print(score.shape, bboxes.shape)\\n        \\n    return bboxes, score \""},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''imgs = list(set(os.listdir(root + 'train')).difference(\\\n                                                       set(list(\\\n                                                                map(lambda arg: arg + '.jpg', \\\n                                                                    list(train['image_id'].unique()))))))\n'''\n\n\n# 00b5c6764.jpg 155c440e8.jpg d771bb084.jpg ed00a614d.jpg c18c32da0.jpg\n# 0cf7ef43d.jpg","execution_count":33,"outputs":[{"output_type":"execute_result","execution_count":33,"data":{"text/plain":"\"imgs = list(set(os.listdir(root + 'train')).difference(                                                       set(list(                                                                map(lambda arg: arg + '.jpg',                                                                     list(train['image_id'].unique()))))))\\n\""},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bb1c3ce8f.jpg ca5e51e59.jpg","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''idx = 2016\nimg = imgs[idx]\nprint(img)\nbbox = train[(train['image_id'] == img.split('.')[0])]['bbox']\nbbox = list(bbox.values)\nbbox = list(map(lambda arg: eval(arg), bbox))\nimg_path = os.path.join(root, 'train', img)\npred, _ = get_prediction(img_path, th=0.5)\nprint(len(bbox))\nimg = viz_bbox(img_path, bbox)\nimg = viz_bbox(img, pred, box_color=(0, 0, 255))\nplt.figure(figsize=(10, 10))\nplt.imshow(img)\nplt.show()\n\nevaluate = Evaluator()\nevaluate.update(idx, pred, bbox)\nevaluate.summary()\n'''\n'''plt.figure(figsize=(15, 15))\nplt.subplot(121)\nplt.imshow(plt.imread(img_path))\nplt.subplot(122)\nplt.imshow(viz_bbox(img_path, pred))\nplt.show()'''","execution_count":35,"outputs":[{"output_type":"execute_result","execution_count":35,"data":{"text/plain":"'plt.figure(figsize=(15, 15))\\nplt.subplot(121)\\nplt.imshow(plt.imread(img_path))\\nplt.subplot(122)\\nplt.imshow(viz_bbox(img_path, pred))\\nplt.show()'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''annotations = {'image': get_img(img_path)}'''","execution_count":36,"outputs":[{"output_type":"execute_result","execution_count":36,"data":{"text/plain":"\"annotations = {'image': get_img(img_path)}\""},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''aug = A.Compose([A.Crop(p=1, x_min=512, y_min=512, x_max=1024, y_max=1024)])\naugmented = aug(**annotations)\n\nplt.figure(figsize=(10, 10))\nplt.imshow(augmented['image'])\nplt.show()'''\n","execution_count":37,"outputs":[{"output_type":"execute_result","execution_count":37,"data":{"text/plain":"\"aug = A.Compose([A.Crop(p=1, x_min=512, y_min=512, x_max=1024, y_max=1024)])\\naugmented = aug(**annotations)\\n\\nplt.figure(figsize=(10, 10))\\nplt.imshow(augmented['image'])\\nplt.show()\""},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''img = torch.from_numpy(augmented['image'].astype(np.float32) / 255.)\nimg = img.permute(2, 0, 1)\npred = get_prediction(img)'''","execution_count":38,"outputs":[{"output_type":"execute_result","execution_count":38,"data":{"text/plain":"\"img = torch.from_numpy(augmented['image'].astype(np.float32) / 255.)\\nimg = img.permute(2, 0, 1)\\npred = get_prediction(img)\""},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''pred'''","execution_count":39,"outputs":[{"output_type":"execute_result","execution_count":39,"data":{"text/plain":"'pred'"},"metadata":{}}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}